{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit6f3bf968d8cf4b29ae3c8e8cca986343",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codice dell'agente DQN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, env, layers=[24,48], C=30, alpha= 0.001, epsilon=1, gamma=0.99, epsilon_reduction=0.05):\n",
    "        #variabili dell'ambiente\n",
    "        self.env = env\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_shape = self.env.observation_space.shape\n",
    "\n",
    "        #replay memory e campione\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.mem_sample = 32\n",
    "\n",
    "        #learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #exploration\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_reduction = epsilon_reduction\n",
    "\n",
    "        #futuro \n",
    "        self.gamma = gamma\n",
    "\n",
    "        #creazione delle due reti neurali, quella dell'addestramento e quella stabile\n",
    "        self.layers = layers\n",
    "        self.train_model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.train_model.get_weights())\n",
    "        self.C = C\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        #Primo layer che accetta lo stato come input\n",
    "        model.add(Dense(self.layers[0], activation='relu', input_shape=self.state_shape))\n",
    "        #Layer nascosti\n",
    "        for l in self.layers:\n",
    "            model.add(Dense(l, activation='relu'))\n",
    "        #Layer di output\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.alpha))\n",
    "\n",
    "        return model\n",
    "\n",
    "    #salvataggio del cervellone\n",
    "    def save_model(self, name=\"cervello_positronico\"):\n",
    "        self.train_model.save(name)\n",
    "        print(\"___Model Saved___\")\n",
    "\n",
    "    #caricamento del cervellone\n",
    "    def load_model(self, name=\"cervello_positronico\"):\n",
    "        self.target_model.load_weights(name)\n",
    "        self.train_model.load_weights(name)\n",
    "        print(\"___Model Loaded___\")\n",
    "\n",
    "    #scelta dell'azione in base ad epsilon\n",
    "    def choose_action(self, state):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.train_model.predict(state)[0])\n",
    "\n",
    "    #repaly memory\n",
    "    def train_from_memory(self):\n",
    "        if len(self.memory) < self.mem_sample:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.mem_sample)\n",
    "\n",
    "        states = []\n",
    "        new_states=[]\n",
    "\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            states.append(state)\n",
    "            new_states.append(new_state)\n",
    "        \n",
    "        na = np.array(states)\n",
    "        states = na.reshape(self.mem_sample, self.state_size)\n",
    "        na2 = np.array(new_states)\n",
    "        new_states = na2.reshape(self.mem_sample, self.state_size)\n",
    "\n",
    "        targets = self.train_model.predict(states)\n",
    "        new_state_targets=self.target_model.predict(new_states)\n",
    "\n",
    "        i=0\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            target = targets[i]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * max(new_state_targets[i])\n",
    "            i+=1\n",
    "        \n",
    "        self.train_model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    #memorizzazione degli stati\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #esecuzione di episodi per l'addestramento\n",
    "    def fit(self, episode, state, render):\n",
    "        total_reward = 0\n",
    "        epochs = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render and episode % 50 == 0:\n",
    "                self.env.render()\n",
    "            \n",
    "            action = self.choose_action(state)\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            new_state = new_state.reshape(1,self.state_size)\n",
    "\n",
    "            self.memorize(state, action, reward, new_state, done)\n",
    "            self.train_from_memory()\n",
    "\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "            epochs += 1\n",
    "\n",
    "        if reward >= 20:\n",
    "            print(\"Success in epsoide {}, used {} iterations!\".format(episode,epochs))\n",
    "            self.save_model()\n",
    "        else:\n",
    "            print(\"Failed in episode {}.\".format(episode))\n",
    "\n",
    "        if episode % self.C:\n",
    "            self.target_model.set_weights(self.train_model.get_weights())\n",
    "\n",
    "        print(\"--now epsilon is {:.3f}, the reward is {}.\".format(max(self.epsilon, self.epsilon_min),total_reward))\n",
    "        if episode > 300:\n",
    "            self.epsilon -= self.epsilon_reduction\n",
    "\n",
    "        return total_reward, epochs\n",
    "    \n",
    "    #chiamata di piÃ¹ episodi per l'addestramento\n",
    "    def start_training(self, episodes=400, render=False):\n",
    "        total_rewards=[]\n",
    "        total_epochs=[]\n",
    "        \n",
    "        try:\n",
    "            for episode in range(episodes):\n",
    "                state = env.reset().reshape(1,self.state_size)\n",
    "                total_reward, epoch = self.fit(episode, state, render)\n",
    "\n",
    "                total_epochs.append(epoch+1)\n",
    "                total_rewards.append(total_reward)\n",
    "        finally:\n",
    "            self.env.close()\n",
    "            self.save_model()\n",
    "\n",
    "        return total_rewards, total_epochs\n",
    "\n",
    "    #test dell'agente\n",
    "    def play(self, filename, trials=400, render=True):\n",
    "        epochs = []\n",
    "        successes = []\n",
    "        total_rewards = []\n",
    "        self.load_model(filename)\n",
    "        try:\n",
    "            for episode in range(trials):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, (1, self.state_size))\n",
    "\n",
    "                succ = 0\n",
    "                i=0\n",
    "                done=False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "                    action = np.argmax(self.train_model.predict(state)[0])\n",
    "\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state = np.reshape(next_state, (1, self.state_size))\n",
    "                    total_reward += reward\n",
    "                    if total_reward >= 19:\n",
    "                        succ = 1\n",
    "                    state = next_state\n",
    "                print(\"Completed/Episodes {}/{}, reward = {}\".format(episode + 1, trials, total_reward))\n",
    "                successes.append(succ)\n",
    "                epochs.append(i+1)\n",
    "                total_rewards.append(total_reward)\n",
    "        finally:\n",
    "            self.env.close()\n",
    "\n",
    "        return epochs, successes, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione dell'ambiente\n",
    "import gym\n",
    "env = gym.make('BipedalWalker-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-60465ce06934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test con un ambiente a caso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpippo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpippo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8a9b5152f18b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, layers, C, alpha, epsilon, gamma, epsilon_reduction)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Test con un ambiente a caso\n",
    "pippo=DQNAgent(env)\n",
    "episodes = 10\n",
    "total_rewards, total_epochs = pippo.start_training(episodes,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}