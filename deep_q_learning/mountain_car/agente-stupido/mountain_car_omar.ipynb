{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitvenvvirtualenv2d5dc0c5be5943f59c3802978ddca48a",
   "display_name": "Python 3.7.6 64-bit ('venv': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codice dell'agente DQN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, env, iterations=200, load=False, alpha= 0.001, epsilon=1.0, gamma=0.99, epsilon_reduction=0.05, model_name=\"cervello_positronico\"):\n",
    "        #variabili dell'ambiente\n",
    "        self.env = env\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_shape = self.env.observation_space.shape\n",
    "        self.max_iterations = iterations\n",
    "\n",
    "        #replay memory e campione\n",
    "        self.memory = deque(maxlen=20_000)\n",
    "        self.mem_sample = 32\n",
    "\n",
    "        #learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #exploration\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_reduction = epsilon_reduction\n",
    "\n",
    "        #futuro \n",
    "        self.gamma = gamma\n",
    "\n",
    "        #creazione delle due reti neurali, quella dell'addestramento e quella stabile\n",
    "        self.model_name = model_name\n",
    "        self.train_model = self.create_model(load)\n",
    "        self.target_model = self.create_model(False)\n",
    "        self.target_model.set_weights(self.train_model.get_weights())\n",
    "\n",
    "    def create_model(self, load):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=self.state_shape, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.alpha))\n",
    "        if load:\n",
    "            model.load_weights(self.model_name)\n",
    "\n",
    "        return model\n",
    "\n",
    "    #scelta dell'azione in base ad epsilon\n",
    "    def choose_action(self, state):\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.train_model.predict(state)[0])\n",
    "\n",
    "    def train_from_memory(self):\n",
    "        if len(self.memory) < self.mem_sample:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.mem_sample)\n",
    "\n",
    "        states = []\n",
    "        new_states=[]\n",
    "\n",
    "        for state, _, _, new_state, _ in minibatch:\n",
    "            states.append(state)\n",
    "            new_states.append(new_state)\n",
    "        \n",
    "        states = np.array(states).reshape(self.mem_sample, 2)\n",
    "        new_states = np.array(new_states).reshape(self.mem_sample, 2)\n",
    "\n",
    "        targets = self.train_model.predict(states)\n",
    "        new_state_targets=self.target_model.predict(new_states)\n",
    "\n",
    "        i=0\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            target = targets[i]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * np.max(new_state_targets[i])\n",
    "            i+=1\n",
    "        \n",
    "        self.train_model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    #salvataggio del cervellone\n",
    "    def save_model(self, name):\n",
    "        self.train_model.save(name)\n",
    "        print(\"___Model Saved___\")\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.target_model.load_weights(name)\n",
    "        self.train_model.load_weights(name)\n",
    "        print(\"___Model Loaded___\")\n",
    "\n",
    "    #memorizzazione degli stati\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def fit(self, episode, state, render):\n",
    "        total_reward = 0\n",
    "        record = self.env.observation_space.low[0]\n",
    "\n",
    "        for epoch in range(self.max_iterations):\n",
    "            if render and episode % 50 == 0:\n",
    "                self.env.render()\n",
    "            \n",
    "            action = self.choose_action(state)\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            new_state = state.reshape(1,self.state_size)\n",
    "\n",
    "            if new_state[0][0] > record:\n",
    "                record = new_state[0][0]\n",
    "\n",
    "            if new_state[0][0] >= 0.5:\n",
    "                reward += 10\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            self.memorize(state, action, reward, new_state, done)\n",
    "            self.train_from_memory()\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if epoch >= self.max_iterations - 1:\n",
    "            print(\"Failed to finish task in epsoide {}\".format(episode))\n",
    "        else:\n",
    "            print(\"Success in epsoide {}, used {} iterations!\".format(episode,epoch))\n",
    "            self.save_model('cervelli/train_model_ep-{}.h5'.format(episode))\n",
    "\n",
    "        #Sync\n",
    "        self.target_model.set_weights(self.train_model.get_weights())\n",
    "\n",
    "        print(\"--now epsilon is {}, the reward is {} maxPosition is {}\".format(max(self.epsilon, self.epsilon_min), total_reward,record))\n",
    "\n",
    "        self.epsilon -= self.epsilon_reduction\n",
    "\n",
    "        return total_reward, epoch\n",
    "    \n",
    "    def start_training(self, episodes=400, render=False):\n",
    "        total_rewards=[]\n",
    "        total_epochs=[]\n",
    "        \n",
    "        try:\n",
    "            for episode in range(episodes):\n",
    "                state = env.reset().reshape(1,self.state_size)\n",
    "                total_reward, epoch = self.fit(episode, state, render)\n",
    "\n",
    "                total_epochs.append(epoch+1)\n",
    "                total_rewards.append(total_reward)\n",
    "        finally:\n",
    "            self.env.close()\n",
    "\n",
    "        return total_rewards, total_epochs\n",
    "\n",
    "    def play(self, trials, filename, render=True):\n",
    "        epochs = []\n",
    "        done = False\n",
    "        successes = []\n",
    "        self.load_model(filename)\n",
    "        try:\n",
    "            for episode in range(trials):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, (1, self.state_size))\n",
    "\n",
    "                succ = 0\n",
    "                for i in range(self.max_iterations):\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "                    action = np.argmax(self.train_model.predict(state))\n",
    "\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state = np.reshape(next_state, (1, self.state_size))\n",
    "\n",
    "                    if next_state[0][0] >= 0.5:\n",
    "                        succ = 1\n",
    "                    state = next_state\n",
    "                print(\"Completed/Episodes {}/{}, success = {}\".format(episode + 1, trials, succ))\n",
    "                successes.append(succ)\n",
    "                epochs.append(i+1)\n",
    "        finally:\n",
    "            self.env.close()\n",
    "\n",
    "        return epochs, successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#creazione dell'ambiente e dell'agente\n",
    "env = gym.make('MountainCar-v0')\n",
    "pippo = DQNAgent(env, alpha=0.001, epsilon_reduction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addestramento del pilota Pippo\n",
    "episodes = 500\n",
    "total_rewards, total_epochs = pippo.start_training(episodes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Epochs\")\n",
    "plt.plot(range(episodes), total_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.plot(range(episodes), total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "___Model Loaded___\nCompleted/Episodes 1/30, success = 0\nCompleted/Episodes 2/30, success = 0\nCompleted/Episodes 3/30, success = 0\nCompleted/Episodes 4/30, success = 0\nCompleted/Episodes 5/30, success = 0\nCompleted/Episodes 6/30, success = 0\nCompleted/Episodes 7/30, success = 0\nCompleted/Episodes 8/30, success = 0\nCompleted/Episodes 9/30, success = 0\nCompleted/Episodes 10/30, success = 0\nCompleted/Episodes 11/30, success = 0\nCompleted/Episodes 12/30, success = 0\nCompleted/Episodes 13/30, success = 0\nCompleted/Episodes 14/30, success = 0\nCompleted/Episodes 15/30, success = 0\nCompleted/Episodes 16/30, success = 0\nCompleted/Episodes 17/30, success = 0\nCompleted/Episodes 18/30, success = 0\nCompleted/Episodes 19/30, success = 0\nCompleted/Episodes 20/30, success = 0\nCompleted/Episodes 21/30, success = 0\nCompleted/Episodes 22/30, success = 0\nCompleted/Episodes 23/30, success = 0\nCompleted/Episodes 24/30, success = 0\nCompleted/Episodes 25/30, success = 0\nCompleted/Episodes 26/30, success = 0\nCompleted/Episodes 27/30, success = 0\nCompleted/Episodes 28/30, success = 0\nCompleted/Episodes 29/30, success = 0\nCompleted/Episodes 30/30, success = 0\n"
    }
   ],
   "source": [
    "#test del pilota\n",
    "pippo = DQNAgent(env, epsilon=0.0)\n",
    "trials = 30\n",
    "epochs, successes = pippo.play(trials , \"pippo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Epoches\")\n",
    "plt.plot(range(trials), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Successes\")\n",
    "plt.plot(range(len(successes)), successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}